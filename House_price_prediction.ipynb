# Importing libraries

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression,Lasso,Ridge
from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV
from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score
from sklearn.preprocessing import MinMaxScaler

import seaborn as sns
import matplotlib.pyplot as plt
import gc


import warnings
warnings.filterwarnings('ignore')

# Reading dataset

df = pd.read_csv("training_set.csv")
df

df.head(5)

# Missing values handling

df.info()

df.isnull().sum()

df.isnull().sum()[0:60]

df.isnull().sum()[60:]

column_names = df.columns
list(column_names)

for i in column_names:
    if df[i].dtypes == 'object':
        df[i].fillna(df[i].mode()[0],inplace=True)
    else:
        df[i].fillna(df[i].mean(),inplace=True)

df.info()

# Outliers Handling

df

cat = []
con = []
for i in df.columns:
    if df[i].dtypes=='object':
        cat.append(i)
    else:
        con.append(i)

con

len(con)

cat

len(cat)

df[con]

from scipy.stats import zscore

columns_to_exclude = ['Id']

# Drop the 'Id' column if it exists
df_without_id = df.drop(columns=columns_to_exclude, errors='ignore')

zscore(df[con])

zscore(df['LotFrontage'])

np.where(np.abs(zscore(df['LotFrontage'])>3))[0].tolist()

out = []
for i in con:
    out = out + list(df[(zscore(df[i])>3)|(zscore(df[i])<-3)].index)
out

outliers = np.unique(out)
outliers

len(outliers)

# Remove the outliers from the dataset
df.drop(outliers,axis=0,inplace=True)

df[0:60]

df.shape

# Reinitialize the index
df.index = range(0,1015,1)

df

# EDA

 select best con and best cat column

plt.figure(figsize=(9, 8))
sns.distplot(df['SalePrice'], color='g', bins=100, hist_kws={'alpha': 0.4});

df["SalePrice_Range"] = pd.cut(df["SalePrice"], 
                                 bins=np.array([-np.inf, 100, 150, 200, np.inf])*1000, 
                                 labels=["0-100k","100k-150k","150k-200k","200k+"])

def plot_target(df: pd.DataFrame, col: str, title: str, pie_colors:list) -> None:
    fig, ax = plt.subplots(1,2,figsize=(15, 6), width_ratios=[2,1])

    textprops={'fontsize': 12, 'weight': 'bold',"color": "black"}
    ax[0].pie(df[col].value_counts().to_list(),
            colors=pie_colors,
            labels=df[col].value_counts().index.to_list(),
            autopct='%1.f%%', 
            explode=([.05]*(df[col].nunique()-1)+[.5]),
            pctdistance=0.5,
            wedgeprops={'linewidth' : 1, 'edgecolor' : 'black'}, 
            textprops=textprops)

    sns.countplot(x = col, data=df, palette = "twilight_shifted", order=df[col].value_counts().to_dict().keys())
    for p, count in enumerate(df[col].value_counts().to_dict().values(),0):
        ax[1].text(p-0.17, count+(np.sqrt(count)), count, color='black', fontsize=13)
    plt.setp(ax[1].get_xticklabels(), fontweight="bold")
    plt.yticks([])
    plt.box(False)
    fig.suptitle(x=0.56, t=f'► {title} Distribution ◄', fontsize=18, fontweight='bold')
    plt.tight_layout()
    plt.show()

plot_target(df, 
            col="SalePrice_Range", 
            title="SalePrice", 
            pie_colors=["#6864a6","#acbec6","#c9b3a3","#914757","#f3f3af","#c0ebe9"])

 Comment:
We created a temporary categorical variable to check the distribution of the target variable.
As seen, our dataset predominantly contains houses from the middle and upper segments.

target = "SalePrice"

plt.figure(figsize=(14,len(cat)*3))
for idx,column in enumerate(cat):
    data = df.groupby(column)[target].mean().reset_index().sort_values(by=target)
    plt.subplot(len(cat)//2+1,2,idx+1)
    sns.barplot(y=column, x=target, data=data, palette="twilight_shifted")
    for p, count in enumerate(data[target].values,0):
        plt.text(count + 10, p+0.05/len(data), f"{int(count//1000)}k", color='black', fontsize=11)
    plt.title(f"{column} and {target}")
    plt.xticks(fontweight='bold')
    plt.box(False)
    plt.tight_layout()

Comment:
We can observe the price distributions across categorical variables through this visualization. It's evident how the prices are influenced by certain unique features of houses.

plt.figure(figsize=(14,len(con)*3))
for idx,column in enumerate(con):
    plt.subplot(len(con)//2+1,2,idx+1)
    sns.boxplot(x="SalePrice_Range", y=column, data=df,palette="twilight_shifted")
    plt.title(f"{column} Distribution")
    plt.tight_layout()

df.corr()['SalePrice'].sort_values()

Q = pd.DataFrame(df.corr()['SalePrice'].sort_values())
imp_con_columns = Q[(Q['SalePrice']>0.2)|(Q['SalePrice']<-0.2)].index.tolist()
imp_con_columns.remove('SalePrice')

imp_con_columns

df[imp_con_columns]


num_corr = df.select_dtypes('number').corr()
sale_corr = num_corr[(num_corr>=0.2) | (num_corr<= -0.2)]
sale_corr = sale_corr[['SalePrice']].fillna(0).sort_values(by='SalePrice')
del num_corr
gc.collect()

# drop the SalePrice from the correlation dataframe
sale_corr = sale_corr[sale_corr.SalePrice != 0].drop('SalePrice', axis=0)

print('Following are the columns and their correlation coefficients with the column "SalePrice" (in descending order):')
sns.heatmap(sale_corr)


corr_matrix = df[imp_con_columns].corr()
corr_matrix

# Correlatioin heatmap

plt.figure(figsize=(20,15))
sns.heatmap(corr_matrix, annot=corr_matrix.values, cmap="Blues", fmt=".2f")
plt.show()

len(cat)

plt.figure(figsize=(30,80))
x = 1
for i in cat:
    plt.subplot(11,4,x)    # 11 rows and 4 columns
    sns.boxplot(x = df[i],y = df['SalePrice'])
    x = x+1

I will consider all the categorical columns

# Preprocessing

X = df.drop(['SalePrice'],axis=1)
Y = df['SalePrice']

X

Y

 X[imp_con_columns]

# Scaling of all the continuous columns >> Normalization
normal_scaler = MinMaxScaler()
array = normal_scaler.fit_transform(X[imp_con_columns])
X1 = pd.DataFrame(array,columns=imp_con_columns)
X1

# One hot encoding of categorical columns
X2 = pd.get_dummies(X[cat])
X2

# join x1 and x2
Xnew = X1.join(X2)
Xnew

# Splitting the dataset

xtrain,xtest,ytrain,ytest = train_test_split(Xnew,Y,test_size=0.25,random_state=21)

xtrain.shape

# Feature selection

# OlS regression model
from statsmodels.api import OLS,add_constant

# ypred = m1x1 + m2x2 + m3x3 + ..... mnxn + c

xconst = add_constant(xtrain,has_constant='add')
ols = OLS(ytrain,xconst).fit()
ols.summary()

100 features

ols.pvalues.sort_values()

ols.pvalues.sort_values().index

ols.rsquared_adj

rsq = round(ols.rsquared_adj,4)
col_to_drop = ols.pvalues.sort_values().index[-1]
rsq,col_to_drop

Xnew = Xnew.drop(col_to_drop,axis=1)   

xtrain,xtest,ytrain,ytest = train_test_split(Xnew,Y,test_size=0.25,random_state=21)

xconst = add_constant(xtrain,has_constant='add')
ols = OLS(ytrain,xconst).fit()
rsq = round(ols.rsquared_adj,4)
col_to_drop = ols.pvalues.sort_values().index[-1]
rsq,col_to_drop

r = []
for i in range(0,150,1):
    Xnew = Xnew.drop(col_to_drop,axis=1)
    xtrain,xtest,ytrain,ytest = train_test_split(Xnew,Y,test_size=0.25,random_state=21)
    xconst = add_constant(xtrain,has_constant='add')
    ols = OLS(ytrain,xconst).fit()
    rsq = round(ols.rsquared_adj,4)
    col_to_drop = ols.pvalues.sort_values().index[-1]
    if col_to_drop == 'const':
        col_to_drop = ols.pvalues.sort_values().index[-2]
    r.append(rsq)

Xnew

import pandas as pd

# Assuming 'data' is your DataFrame
correlation_matrix = Xnew.corr()
correlation_matrix

a = Xnew.columns
a

len(Xnew.columns)

# Model training

lin_model = LinearRegression()
lin_model.fit(xtrain,ytrain)

# Model evaluation

# Testing data evaluation
ypredtest = lin_model.predict(xtest)
# ypredtest

Mean_absolute_error = mean_absolute_error(ytest,ypredtest)
print('Mean_absolute_error:',Mean_absolute_error)

Mean_squared_error = mean_squared_error(ytest,ypredtest)
print('Mean_squared_error:',Mean_squared_error)

R_squared = r2_score(ytest,ypredtest)
print('R_squared:',R_squared)

Adj_R2 = 1- ((1-R_squared)*(len(xtest) -1) / (len(xtest)-len(Xnew.columns)-1))
print('Adj_R2:',Adj_R2)

# Training data evaluation
ypredtrain = lin_model.predict(xtrain)
# ypredtest

Mean_absolute_error = mean_absolute_error(ytrain,ypredtrain)
print('Mean_absolute_error:',Mean_absolute_error)

Mean_squared_error = mean_squared_error(ytrain,ypredtrain)
print('Mean_squared_error:',Mean_squared_error)

R_squared = r2_score(ytrain,ypredtrain)
print('R_squared:',R_squared)

Adj_R2 = 1- ((1-R_squared)*(len(xtrain) -1) / (len(xtrain)-len(Xnew.columns)-1))
print('Adj_R2:',Adj_R2)

# Build a regularization model

# Without hyperparameter tuning

# 1.Ridge regression

ridge_model = Ridge()   # alpha=1.0
ridge_model.fit(xtrain,ytrain)

# Testing data evaluation
ypredtest = ridge_model.predict(xtest)
# ypredtest

Mean_absolute_error = mean_absolute_error(ytest,ypredtest)
print('Mean_absolute_error:',Mean_absolute_error)

Mean_squared_error = mean_squared_error(ytest,ypredtest)
print('Mean_squared_error:',Mean_squared_error)

R_squared = r2_score(ytest,ypredtest)
print('R_squared:',R_squared)

Adj_R2 = 1- ((1-R_squared)*(len(xtest) -1) / (len(xtest)-len(Xnew.columns)-1))
print('Adj_R2:',Adj_R2)


# Training data evaluation
ypredtrain = ridge_model.predict(xtrain)
# ypredtest

Mean_absolute_error = mean_absolute_error(ytrain,ypredtrain)
print('Mean_absolute_error:',Mean_absolute_error)

Mean_squared_error = mean_squared_error(ytrain,ypredtrain)
print('Mean_squared_error:',Mean_squared_error)

R_squared = r2_score(ytrain,ypredtrain)
print('R_squared:',R_squared)

Adj_R2 = 1- ((1-R_squared)*(len(xtrain) -1) / (len(xtrain)-len(Xnew.columns)-1))
print('Adj_R2:',Adj_R2)


# 2.Lasso regression

lasso_model = Lasso()
lasso_model.fit(xtrain,ytrain)

# Testing data evaluation
ypredtest = lasso_model.predict(xtest)
# ypredtest

Mean_absolute_error = mean_absolute_error(ytest,ypredtest)
print('Mean_absolute_error:',Mean_absolute_error)

Mean_squared_error = mean_squared_error(ytest,ypredtest)
print('Mean_squared_error:',Mean_squared_error)

R_squared = r2_score(ytest,ypredtest)
print('R_squared:',R_squared)

Adj_R2 = 1- ((1-R_squared)*(len(xtest) -1) / (len(xtest)-len(Xnew.columns)-1))
print('Adj_R2:',Adj_R2)

# Training data evaluation
ypredtrain = lasso_model.predict(xtrain)

Mean_absolute_error = mean_absolute_error(ytrain,ypredtrain)
print('Mean_absolute_error:',Mean_absolute_error)

Mean_squared_error = mean_squared_error(ytrain,ypredtrain)
print('Mean_squared_error:',Mean_squared_error)

R_squared = r2_score(ytrain,ypredtrain)
print('R_squared:',R_squared)

Adj_R2 = 1- ((1-R_squared)*(len(xtrain) -1) / (len(xtrain)-len(Xnew.columns)-1))
print('Adj_R2:',Adj_R2)

# With hyperparameter tuning

# 1. Ridge with GridsearchCV

ridge_model = Ridge()

hyp_grid = {'alpha':np.arange(0.1,1,0.01)}

gscv_ridge_model = GridSearchCV(ridge_model,hyp_grid,cv = 5)

gscv_ridge_model.fit(xtrain,ytrain)

gscv_ridge_model.best_estimator_

ridge_model = Ridge(alpha=0.1)  
ridge_model.fit(xtrain,ytrain)

# Testing data evaluation
ypredtest = ridge_model.predict(xtest)

Mean_absolute_error = mean_absolute_error(ytest,ypredtest)
print('Mean_absolute_error:',Mean_absolute_error)

Mean_squared_error = mean_squared_error(ytest,ypredtest)
print('Mean_squared_error:',Mean_squared_error)

R_squared = r2_score(ytest,ypredtest)
print('R_squared:',R_squared)

Adj_R2 = 1- ((1-R_squared)*(len(xtest) -1) / (len(xtest)-len(Xnew.columns)-1))
print('Adj_R2:',Adj_R2)

# Training data evaluation
ypredtrain = ridge_model.predict(xtrain)

Mean_absolute_error = mean_absolute_error(ytrain,ypredtrain)
print('Mean_absolute_error:',Mean_absolute_error)

Mean_squared_error = mean_squared_error(ytrain,ypredtrain)
print('Mean_squared_error:',Mean_squared_error)

R_squared = r2_score(ytrain,ypredtrain)
print('R_squared:',R_squared)

Adj_R2 = 1- ((1-R_squared)*(len(xtrain) -1) / (len(xtrain)-len(Xnew.columns)-1))
print('Adj_R2:',Adj_R2)

# 2. Lasso with GridsearchCV

lasso_model = Lasso()

hyp_grid = {'alpha':np.arange(0.1,1,0.01)}

gscv_lasso_model = GridSearchCV(lasso_model,hyp_grid,cv = 5)

gscv_lasso_model.fit(xtrain,ytrain)

gscv_lasso_model.best_estimator_

lasso_model = Lasso(alpha=0.9899999999999995)
lasso_model.fit(xtrain,ytrain)

# Testing data evaluation
ypredtest = lasso_model.predict(xtest)
# ypredtest

Mean_absolute_error = mean_absolute_error(ytest,ypredtest)
print('Mean_absolute_error:',Mean_absolute_error)

Mean_squared_error = mean_squared_error(ytest,ypredtest)
print('Mean_squared_error:',Mean_squared_error)

R_squared = r2_score(ytest,ypredtest)
print('R_squared:',R_squared)

Adj_R2 = 1- ((1-R_squared)*(len(xtest) -1) / (len(xtest)-len(Xnew.columns)-1))
print('Adj_R2:',Adj_R2)

# Training data evaluation
ypredtrain = lasso_model.predict(xtrain)

Mean_absolute_error = mean_absolute_error(ytrain,ypredtrain)
print('Mean_absolute_error:',Mean_absolute_error)

Mean_squared_error = mean_squared_error(ytrain,ypredtrain)
print('Mean_squared_error:',Mean_squared_error)

R_squared = r2_score(ytrain,ypredtrain)
print('R_squared:',R_squared)

Adj_R2 = 1- ((1-R_squared)*(len(xtrain) -1) / (len(xtrain)-len(Xnew.columns)-1))
print('Adj_R2:',Adj_R2)

# Final model

# Best performing model
ridge_model = Ridge()   # alpha=1.0
ridge_model.fit(Xnew,Y)

df1 = pd.read_csv('testing_set.csv')
df1

df1.isnull().sum()

df1.isnull().sum()[0:60]

df1.isnull().sum()[60:]

df1.isna().sum()[0:60]

df.isna().sum()[60:]

df1['LotFrontage'].mean()

df1['MSZoning'].mode()[0]

list(df1.columns)

df1.info()

for i in df1.columns:
    if df1[i].dtypes == 'object':
        df1[i].fillna(df1[i].mode()[0],inplace=True)
    else:
        df1[i].fillna(df1[i].mean(),inplace=True)

df1.isna().sum()[:60]

df1.isna().sum()[60:]

# Outliers handling

cat = []
con = []
for i in df1.columns:
    if df1[i].dtypes=='object':
        cat.append(i)
    else:
        con.append(i)

con

cat

len(con)

len(cat)

df1[con]

from scipy.stats import zscore

zscore(df1[con])

zscore(df1["LotFrontage"])

np.where(np.abs(zscore(df1["LotFrontage"])>3))[0].tolist()

out = []
for i in con:
    out = out + list(df1[(zscore(df1[i])>3)|(zscore(df1[i])<-3)].index)
out

outliers = np.unique(out)
outliers

len(outliers)

# Remove the outliers from the dataset
df1.drop(outliers,axis=0,inplace=True)

df1[0:60]

df1.shape

# Reinitialize the index
df1.index = range(0,1019,1)

len(a)

submission_df = Xnew[a]
submission_df

submission_df.columns.values

Q = ridge_model.predict(submission_df)
Q

submission_df = submission_df.join(pd.Series(Q).to_frame('SalePrice_pred'), 
                                   lsuffix='',rsuffix='')
submission_df

submission_df[[ 'SalePrice_pred']].head()

Q = Q.reshape(-1)
Q = pd.DataFrame({
    'Id': range(1461, 1461 + len(Q.flatten())),
    'SalePrice': Q.flatten()
})
Q = Q.set_index('Id')
Q


import pandas as pd

# Assuming df is your DataFrame
# Set the display option to show all rows
pd.set_option('display.max_rows', None)

# Now, when you print your DataFrame, it will display all rows
print(Q)
pd.DataFrame(Q)

Q.to_csv("submission.csv")
